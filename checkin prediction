# ***** Prediction for Review Score Location ******
#install.packages("data.table")
#install.packages("caret")
#install.packages("ggplot2")
#install.packages("corrplot")
#install.packages("tree")
#install.packages("e1071")
#install.packages("leaps")
#install.packages("ModelMetrics")
library(corrplot)
library(caret)
library(data.table)
library(ggplot2)
library(ModelMetrics)
rm(list = ls())
setwd("~/Desktop")
#path = '/Users/admin/Downloads/listings.csv'
airbnb = fread(input = "listings.csv")
#airbnb = fread(input = path)

# ***** Preliminary Data Cleaning ******

blank_cols = c("neighbourhood_group_cleansed","calendar_updated","license")
# We will keep the bathroom column for now, as we can extract that data from bathrooms_text

# Removing Blank columns, note how the variables in our data frame drop from 74 to 71
airbnb = subset(airbnb, select = !(names(airbnb) %in% blank_cols))

#Lets store number of bathrooms in the blank column, by extracting data from the bathrooms_text column
airbnb$bathrooms = as.numeric(gsub('[a-zA-Z]', '', airbnb$bathrooms_text))

# Saving host_since as a individual variable
host_since =as.character(airbnb [["host_since"]])
# Calculate the joining time for each host
join_time = c()
for(i in 1: length(host_since )){
  join_time[i] = difftime("2021-12-09", strptime(host_since[i], format = "%Y-%m-%d"), ,unit = "days")
}
# Introduce join_time as a new column into data set
airbnb = cbind(airbnb,join_time)

# Lets remove columns not required for our analysis 
cols_to_be_removed = c("id","listing_url","scrape_id","last_scraped","name","description","neighborhood_overview",
                       "picture_url","host_id","host_url","host_name","host_since","host_about","host_thumbnail_url",
                       "host_picture_url","calendar_last_scraped", "first_review", "last_review", "bathrooms_text", 
                       "host_listings_count","neighbourhood","minimum_minimum_nights",
                       "maximum_minimum_nights","minimum_maximum_nights","maximum_maximum_nights",
                       "minimum_nights_avg_ntm","maximum_nights_avg_ntm")

airbnb = subset(airbnb, select = !(names(airbnb) %in% cols_to_be_removed))

# We see that some columns still have N/A values which are not being treated as such
# because R is reading them as strings, the following code helps us with that - 
airbnb[airbnb=="N/A"] = NA

# We also have some blank values in some columns that should be converted to NA as well
airbnb[airbnb == "" | airbnb == " "] = NA 

# Check NA values as a percentage of total data again 
hist(colMeans(is.na(airbnb)),
     labels = TRUE,
     col = "darkblue",
     main = "NA Values as a percentage of Data",
     xlab = "Mean NA Values",
     border = "white",
     ylim = c(0,65))

# removes listings with no stays or corrupted/incomplete reviews
airbnb = airbnb[!is.na(airbnb$review_scores_location) & 
                  !is.na(airbnb$review_scores_checkin) & 
                  !is.na(airbnb$review_scores_cleanliness) & 
                  !is.na(airbnb$review_scores_value),]

# turns all NA join times into the mean join time
# airbnb$join_time[which(is.na(airbnb$join_time))] = mean(airbnb$join_time[which(!is.na(airbnb$join_time))])

# count number of verification ways
#install.packages("BBmisc")
library(BBmisc)
verification = airbnb [,c("host_verifications")]
counts_verification = c()
for (i in 1: dim(verification)[1]){
  a = toString(verification[i])
  counts_verification[i] = length(explode( a,','))
}
airbnb = cbind(airbnb,counts_verification)


# *** More Data Cleaning ***


# Clean verifications & amenities and count unique
unique_verifications = gsub("[[:punct:]]","",unlist(strsplit(airbnb$host_verifications,split=",")))
unique_verifications = unique(unique_verifications)

clean_host_verifications = strsplit(gsub("[[:punct:]]","",airbnb$host_verifications), split = " ")
airbnb = airbnb[,-c("host_verifications")]

unique_amenities = gsub("[[:punct:]]","",unlist(strsplit(airbnb$amenities,split=",")))
temp = as.factor(unique_amenities) # store factor information before we change unique_amenities
unique_amenities = unique(unique_amenities)

clean_amenities = strsplit(gsub("[[:punct:]]","",airbnb$amenities), split = " ")
airbnb = airbnb[,-c("amenities")]

summary = summary(temp)
for (i in 1:90) { # remove white space that R introduced
  if(substring(names(summary)[i],1,1)==" ") {
    names(summary)[i] = substring(names(summary)[i],2)
  }
}
keepT90 = names(summary)[1:90] # top 90 most frequent ammenities
keepT90 = keepT90[-c(29,32,38,74)] # remove duplicate shampoo washer Hot water heating

# keep only top 90 amenities and discard the rest
#for (i in 1:length(clean_amenities)) {
#  temp_index = clean_amenities[[i]][1:length(clean_amenities[[i]])] %in% keepT90
#  clean_amenities[[i]] = clean_amenities[[i]][temp_index]
#}
# convert to dummy
# create temporary table first of all 0s for every dummy
#temp_table = airbnb[,1:2]
#temp_table[ , keepT90] <- c(0)
#temp_table = temp_table[,3:length(temp_table[1])]

# for every amenity go to the appropriate dummy column and turn that 
# flat's value to 1
#for (i in 1:length(clean_amenities)) {
#  for (j in clean_amenities[[i]]){
#    #if (i %% 1000 == 0) {print(i)}
#    temp_table[[j]][i] = 1 
#  }
#}
# attach the dummy columns to main data table
#airbnb = cbind(airbnb,temp_table)

##clean data for check in score
# we select acceptance rate, response time, superhost, price, instant bookable as attributes
set.seed(1)
check_in = airbnb[, c("host_acceptance_rate","host_response_time","host_is_superhost","price","instant_bookable","review_scores_checkin")]
colMeans(is.na(check_in))
# missing value col:host_acceptance_rate, host_response_time , host_is_superhost
# 45% and 49% missing rate in acceptance rate and response rate respectively
summary(check_in)

# host reponse time: categorical; present na as a new categorial
table(check_in$host_response_time)
# combine small cases into a large group: na,quick,slow
check_in$host_response_time[is.na(check_in$host_response_time)] = "NA"
check_in$host_response_time[check_in$host_response_time == "within an hour"|check_in$host_response_time == "within a few hours"] = "quick"
check_in$host_response_time[check_in$host_response_time == "within a day"|check_in$host_response_time == "a few days or more"] = "slow"


# host_is_superhost; present na as a new categorial
table(check_in$host_is_superhost)# small portion of missing value may carry important information
check_in$host_is_superhost[is.na(check_in$host_is_superhost)] = "NA"
check_in$host_is_superhost[check_in$host_is_superhost == "t"] = 1
check_in$host_is_superhost[check_in$host_is_superhost == "f"] = 0

# price
check_in$price=gsub("\\$", "", check_in$price)
check_in$price=gsub(",", "", check_in$price)
check_in$price = as.numeric(check_in$price )

#instant bookable
check_in$instant_bookable[check_in$instant_bookable == "t"] = 1
check_in$instant_bookable[check_in$instant_bookable == "f"] = 0

#host_acceptance_rate
check_in$host_acceptance_rate = (as.numeric(gsub("%","",check_in$host_acceptance_rate)))/100
#impute unknown by the median in training set
test_set_indices = sample(1:nrow(check_in),round(0.3*nrow(check_in)),replace = FALSE)
training_set = check_in[-test_set_indices,]
test_set = check_in[test_set_indices,]
summary(training_set) # median = 0.910 
training_set$host_acceptance_rate[is.na(training_set$host_acceptance_rate)] = 0.910
test_set$host_acceptance_rate[is.na(test_set$host_acceptance_rate)] = 0.910
# convert all character into factor
library(dplyr)
training_set = training_set %>% mutate_if(sapply(training_set, is.character), as.factor)
test_set = test_set %>% mutate_if(sapply(test_set, is.character), as.factor)
# check
colMeans(is.na(training_set))
colMeans(is.na(test_set))
summary(training_set)
summary(test_set)

# EDA
continuous_var = training_set[,c("host_acceptance_rate","price","review_scores_checkin")]
corrplot(cor(continuous_var), method = "circle", diag = FALSE)
# shows price and acceptance rate both are slightlt negtively correlated with check in score;
# almost no relationship between attributes
training_set %>% ggplot(aes(y=review_scores_checkin, x=factor(host_response_time)))+ geom_boxplot() 
training_set %>% ggplot(aes(y=review_scores_checkin, x=factor(host_is_superhost)))+ geom_boxplot() 
training_set %>% ggplot(aes(y=review_scores_checkin, x=factor(instant_bookable)))+ geom_boxplot() 

install.packages("polywog")
install.packages("rpart.plot")
library(corrplot)
library(caret)
library(data.table)
library(ggplot2)
library(ModelMetrics)
library(polywog)
library (tree)
library(rpart.plot)
library(e1071)
##linear regression
# convert them factor into numeric 
training_set1 = training_set %>% mutate_if(sapply(training_set, is.factor), as.numeric)
test_set1 = test_set %>% mutate_if(sapply(test_set, is.factor), as.numeric)
lm_reg = train( review_scores_checkin~.,
               data =training_set1 ,
                 method = 'lm',
                trControl = trainControl(method = 'repeatedcv' , number = 10, repeats = 3),
               preProcess = c('center','scale'))
lm_reg$finalModel
summary(lm_reg$finalModel)
# all variables are significant,
par(mfrow=c(2,2))
plot(lm_reg$finalModel) # residual plots are bad.
# rmse for training set:  0.4749543; rmse for test set is 0.4459939
rmse(training_set1$review_scores_checkin,predict(lm_reg,training_set1[,-c("review_scores_checkin"),drop=FALSE]))
rmse(test_set1$review_scores_checkin,predict(lm_reg,test_set1[,-c("review_scores_checkin"),drop=FALSE]))

## Decision tree regression
library(tree)
library(rpart.plot)
tree_checkin<- tree (review_scores_checkin~. , data=training_set)
summary(tree_checkin )
plot (tree_checkin)
text (tree_checkin , pretty = 0)
# rmse in training set = 0.4778013; rmse in testing set is 0.4478906
mean ((predict (tree_checkin , newdata =training_set) - training_set$review_scores_checkin)^2) 
mean ((predict (tree_checkin , newdata =test_set) - test_set$review_scores_checkin)^2)
# only care about superhost: if it is superhost, then it would be higher score
# use cv for a more detailed subtree
cp.grid = expand.grid(.cp = (0:10)*0.001)
tree_reg = train(review_scores_checkin~.,
                data = training_set1, method = 'rpart',
                trControl = trainControl(method = 'repeatedcv' , number = 10, repeats = 3),
                tuneGrid = cp.grid )
tree_reg 
best.tree = tree_reg$finalModel
prp(best.tree) # the best cp is 0.001  
# rmse for training set is 0.4719312;
mean ((predict (best.tree , newdata =training_set1) - training_set1$review_scores_checkin)^2) 
# rmse for training set is 0.4440508
mean ((predict (best.tree , newdata =test_set1) - test_set1$review_scores_checkin)^2) 

#log transform check in score perform still not good

## SVM Regression: choose radial kernel
library(caret)
# fit in radial kernal 
# eps-regression: no control of number of support vectors
svmfit1 = svm(review_scores_checkin~. ,data=training_set1, kernel="radial" ,scale=TRUE) # needs around 5 minutes to run
summary(svmfit1) 
# training set rmse = 0.4959456
rmse(training_set1$review_scores_checkin,predict(svmfit1,training_set1)) 
# Testing set rmse = 0.4646491
rmse(test_set1$review_scores_checkin,predict(svmfit1,test_set1))

# fit in linear kernal svm regression
svmfit2 = svm(review_scores_checkin~. ,data=training_set1, kernel="linear" ,scale=TRUE)
# training set rmse = 0.5046351
rmse(training_set1$review_scores_checkin,predict(svmfit2,training_set1))
# testing set rmse = 0.4731245
rmse(test_set1$review_scores_checkin,predict(svmfit2,test_set1))

