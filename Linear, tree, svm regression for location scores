set.seed(1)
airbnb_location = airbnb[,c("latitude","longitude","review_scores_location")]
colMeans(is.na(airbnb_location)) # no missing value
summary(airbnb_location)
library(ggplot2)

# Divide into test dataset 30% and training dataset 70%
test_set_indices = sample(1:nrow(airbnb_location),round(0.3*nrow(airbnb_location)),replace = FALSE)
training_set = airbnb_location[-test_set_indices,]
test_set = airbnb_location[test_set_indices,]

# EDA
hist(training_set$latitude) # lookes like normal distribution
hist(training_set$longitude) # looks like normal distribution
hist(training_set$review_scores_location)# skewed to the right, needs some transformation
# taking log as log_location = log(training_set$review_scores_location)
log_location = log(training_set$review_scores_location)
hist(log_location) # still not normal


pairs(airbnb_location) 
install.packages("corrplot")
library(corrplot)
corrplot(cor(airbnb_location), method = "circle", diag = FALSE,type = "upper")


## Fit in linear regession model (can not be used)
lm1 = lm(review_scores_location ~ latitude + longitude ,data=training_set)
summary(lm1)
par(mfrow=c(2,2))
plot(lm1) # implies not normally distributed

lm2 = lm(review_scores_location ~ longitude ,data=training_set)
summary(lm2)

# Test on test dataset
library(ModelMetrics)
# training set mse = 0.188853 ; rmse = 0.4345722
rmse(training_set$review_scores_location,predict(lm1,training_set[,c("latitude","longitude"),drop=FALSE]))
# test set mse = 0.1607567 ; rmse = 0.4009448
rmse(test_set$review_scores_location,predict(lm1,test_set[,c("latitude","longitude"),drop=FALSE]))

# Model selection
library(leaps)
regfit_full = regsubsets(review_scores_location~. , data=training_set)
summary_full = summary(regfit_full)
# Best way is to keep both variables


## regression tree
library (tree)
tree_location <- tree (review_scores_location~. , data=training_set)
summary(tree_location )
plot (tree_location)
text (tree_location , pretty = 0)
p1 <- predict (tree_location , newdata =test_set)
p2 <- predict (tree_location , newdata =training_set)
mean ((p2 - training_set$review_scores_location)^2) # training data: mse = 0.1873069; rmse = 0.4327897
mean ((p1 - test_set$review_scores_location)^2)  # testing data: mse = 0.1592019; rmse = 0.3990011


## svm regression
# fit in radial kernal 
library(e1071)
plot(training_set[,1:2],col=training_set$review_scores_location+3,asp=0) # circle, suggest using radial kernal
# eps-regression: no control of number of support vectors
svmfit1 = svm(review_scores_location~. ,data=training_set, kernel="radial" ,scale=FALSE) # needs around 1 minutes to run
summary(svmfit) 
library(ModelMetrics)
# training set mse is 0.2021391;rmse = 0.4495988
rmse(training_set$review_scores_location,predict(svmfit1,training_set[,1:2])) 
# Testing set mse is 0.1728991; rmse = 0.4158114
rmse(test_set$review_scores_location,predict(svmfit1,test_set[,1:2]))

# fit in linear kernal svm regression
svmfit2 = svm(review_scores_location~. ,data=training_set, kernel="linear" ,scale=FALSE)
# training set mse is 0.2052773; rmse = 0.4530754
rmse(training_set$review_scores_location,predict(svmfit2,training_set[,1:2]))
# testing set mse is  0.1756722; rmse = 0.4191327
rmse(test_set$review_scores_location,predict(svmfit2,test_set[,1:2]))

# fit in polynomial kernal svm regression
svmfit3 = svm(review_scores_location~. ,data=training_set, kernel="polynomial" ,scale=FALSE)
# training set  rmse = 337.2504
rmse(training_set$review_scores_location,predict(svmfit3,training_set[,1:2]))
# testing set rmse = 337.2485
rmse(test_set$review_scores_location,predict(svmfit3,test_set[,1:2]))
